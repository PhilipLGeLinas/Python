"""

Developed By: Philip L. GeLinas
Date:         1.12.2019
Description:  A machine learning algorithm that estimates the
              result of a simple algebraic calculation.

"""

import random


def main():

    learn()


# Estimation generated by adjustment of
# two weights over 10,000 iterations
def learn():

    # Starting weights
    weight1 = random.randint(1, 10)
    weight2 = random.randint(1, 10)

    for i in range(0, 100000):

        # Randomly generated integers
        a = random.randint(1, 100)
        b = random.randint(1, 100)

        output = a * weight1 + b * weight2
        actual = calculate(a, b)
        error = (actual - output) / actual

        # Adjustments to be made to each weight
        adjustment1 = 0.01 * error * a
        adjustment2 = 0.01 * error * b

        weight1 += adjustment1
        weight2 += adjustment2

    # Test result of algorithm
    c = random.randint(1, 100)
    d = random.randint(1, 100)

    estimate = weight1 * c + weight2 * d
    actual = calculate(c, d)

    print("Value 1 and 2: " + str(c) + ", " + str(d))
    print("Estimated Result: " + str(estimate))
    print("Actual Result: " + str(actual))
    print("Estimate accurate to within " + str(((actual - estimate) / actual) * -1.0) + "%.")


# Calculation performed
def calculate(a, b):

    return (a + b) * 2


if __name__ == '__main__':

    main()
